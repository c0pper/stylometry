{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4HeIgjpN+Drc9Clumxerq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1465ada497f41d69532fa09b7ac77ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15b95819bffd4ee6be83765fb44810e7",
              "IPY_MODEL_44f75837135641e3bc423b4a2a105686",
              "IPY_MODEL_91ad1c55f7d749f4ac8c0e68f9fc1e1e"
            ],
            "layout": "IPY_MODEL_70289188442a4cc9970b9adba39ef24c"
          }
        },
        "15b95819bffd4ee6be83765fb44810e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a48103e3e779420d85cad5e4f19f2820",
            "placeholder": "​",
            "style": "IPY_MODEL_ed4adf9fe6184b129736698071f752b4",
            "value": ""
          }
        },
        "44f75837135641e3bc423b4a2a105686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399aba2c90a0439d991649ac56a7c546",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_051859a1726a491b91c0318b9b07e4ab",
            "value": 0
          }
        },
        "91ad1c55f7d749f4ac8c0e68f9fc1e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e3e832d07ee465baef3d623a686c46d",
            "placeholder": "​",
            "style": "IPY_MODEL_4b27bf0214dc4755a8646ded52314046",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "70289188442a4cc9970b9adba39ef24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a48103e3e779420d85cad5e4f19f2820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4adf9fe6184b129736698071f752b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "399aba2c90a0439d991649ac56a7c546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "051859a1726a491b91c0318b9b07e4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e3e832d07ee465baef3d623a686c46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b27bf0214dc4755a8646ded52314046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c0pper/stylometry/blob/main/expertiment_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9b2u_METnk1",
        "outputId": "2c34acbf-2fc2-4f99-e5df-645deee74e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPUa1BXLFiYe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "c1465ada497f41d69532fa09b7ac77ca",
            "15b95819bffd4ee6be83765fb44810e7",
            "44f75837135641e3bc423b4a2a105686",
            "91ad1c55f7d749f4ac8c0e68f9fc1e1e",
            "70289188442a4cc9970b9adba39ef24c",
            "a48103e3e779420d85cad5e4f19f2820",
            "ed4adf9fe6184b129736698071f752b4",
            "399aba2c90a0439d991649ac56a7c546",
            "051859a1726a491b91c0318b9b07e4ab",
            "1e3e832d07ee465baef3d623a686c46d",
            "4b27bf0214dc4755a8646ded52314046"
          ]
        },
        "outputId": "72c5f69c-8292-4828-cb5c-95562479334c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1465ada497f41d69532fa09b7ac77ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from typing import Union\n",
        "import math \n",
        "from sklearn import preprocessing, metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from requests import get\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from tensorflow.keras.models import load_model\n",
        "import time\n",
        "import json\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "MODEL_SAVEPATH = \"/content/drive/MyDrive/simo/\"\n",
        "TIMENOW = datetime.now().strftime('%d-%m-%y-%H-%M')\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/logs\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/logs/sklearn\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/logs/bert\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/logs/stylo\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/models\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/models/sklearn\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/models/bert\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/simo/models/stylo\", exist_ok=True)\n",
        "\n",
        "def merge_datasets(dataset_list: list, target_col: str):\n",
        "  round_threshold = 0.49\n",
        "  processed_datasets = []\n",
        "  list90 = []\n",
        "  list10 = []\n",
        "\n",
        "  for idx, d in enumerate(dataset_list):\n",
        "    print(f\"original dataset {idx} shape\", d.shape)\n",
        "    original_shape = d.shape[0]\n",
        "\n",
        "    d90 = pd.DataFrame()\n",
        "    d10 = pd.DataFrame()\n",
        "    values_form_target = pd.unique(d[target_col].squeeze())\n",
        "    values_shapes = []\n",
        "    for v in values_form_target:\n",
        "      d_label = d[(d[target_col] == v)]\n",
        "      # print(d_label[target_col])\n",
        "      value_shape = d_label.shape[0]\n",
        "      values_shapes.append(value_shape)\n",
        "\n",
        "      d_label90perc, d_label10perc = np.split(d_label, [int(.9*len(d_label))])\n",
        "      print(f\"shape 90% for {v}: {d_label90perc.shape[0]} == shape*0.9: {value_shape*0.9}\")\n",
        "      print(f\"shape 10% for {v}: {d_label10perc.shape[0]} == shape*0.1: {value_shape*0.1}\")\n",
        "\n",
        "      d90 = d90.append(d_label90perc, ignore_index=True) # unisco i 2 sottodataset contenenti solo label1 e label2\n",
        "      d10 = d10.append(d_label10perc, ignore_index=True) # unisco i 2 sottodataset contenenti solo label1 e label2\n",
        "    assert(sum(values_shapes) == original_shape)\n",
        "    \n",
        "    d90 = d90.sample(frac=1, random_state=42) # mischio le righe per evitare che ci siano prima tutti label1 e poi tutti label2\n",
        "    d10 = d10.sample(frac=1, random_state=42)\n",
        "    assert((d90.shape[0] + d10.shape[0]) == original_shape)\n",
        "\n",
        "    print(\"\\n90% of dataset\\n\", d90.groupby(target_col)[target_col].count())# stampo il conteggio delle classi presenti nella nuova coppia di dataset derivata dall'originale\n",
        "    print(\"\\n10% of dataset\\n\", d10.groupby(target_col)[target_col].count(), \"\\n\\n\\n\")\n",
        "    processed_datasets.append((d90, d10))\n",
        "\n",
        "  for tup in processed_datasets:\n",
        "    df90 = tup[0]\n",
        "    df10 = tup[1]\n",
        "    list90.append(df90)\n",
        "    list10.append(df10)\n",
        "  merged90 = pd.DataFrame()\n",
        "  merged10 = pd.DataFrame()\n",
        "  for df in list90:\n",
        "    merged90 = merged90.append(df)\n",
        "  for df in list10:\n",
        "    merged10 = merged10.append(df)\n",
        "\n",
        "  return(merged90, merged10)\n",
        "\n",
        "def preprocess(text,stem=False):\n",
        "    stop_words = stopwords.words('english')\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    text = text.lower()  # lowercase\n",
        "\n",
        "    text = re.sub(r'[!]+', '!', text)\n",
        "    text = re.sub(r'[?]+', '?', text)\n",
        "    text = re.sub(r'[.]+', '.', text)\n",
        "    text = re.sub(r'â€™', \"'\", text)\n",
        "    text = re.sub(r'â€œ', \"'\", text)\n",
        "    text = re.sub(r'â€', \"'\", text)\n",
        "    text = re.sub(r'â€˜', \"'\", text)\n",
        "    text = re.sub(r'â‚¬', \"€\", text)\n",
        "    text = re.sub(r\"'\", \"\", text)\n",
        "    text = re.sub('\\s+', ' ', text).strip()  # Remove and double spaces\n",
        "    text = re.sub(r'&amp;?', r'and', text)  # replace & -> and\n",
        "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)  # Remove URLs\n",
        "    # remove some puncts (except . ! # ?)\n",
        "    text = re.sub(r'[:\"$%&\\*+,-/:;<=>@\\\\^_`{|}~]+', '', text)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'EMOJI', text)\n",
        "    \n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            tokens.append(lemmatizer.lemmatize(token))\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "def apply_preprocess(xtrain, xvalid):\n",
        "  print(\"\\nPreprocessing texts...\")\n",
        "  print(f\"\\nBefore: {(xtrain.iloc[0][:50] + '..') if len(xtrain.iloc[0]) > 50 else xtrain.iloc[0]}\")\n",
        "  xtrain = xtrain.progress_apply(lambda x: preprocess(x))\n",
        "  xvalid = xvalid.progress_apply(lambda x: preprocess(x))\n",
        "  print(f\"\\nAfter: {(xtrain.iloc[0][:50] + '..') if len(xtrain.iloc[0]) > 50 else xtrain.iloc[0]}\")\n",
        "\n",
        "  return xtrain, xvalid\n",
        "\n",
        "\n",
        "class Experiment:\n",
        "  scaler = StandardScaler()\n",
        "  lbl_enc = preprocessing.LabelEncoder()\n",
        "\n",
        "  def __init__(self, dataset_path: Union[str, pd.DataFrame], split_size: float, target_col: int, model_savepath=MODEL_SAVEPATH):\n",
        "    self.dataset_path = dataset_path\n",
        "    self.split_size = split_size\n",
        "    self.target_col = target_col\n",
        "    if isinstance(self.dataset_path, str):\n",
        "      self.dataset_name = dataset_path.split(\".\")[-2].split(\"/\")[-1]\n",
        "    elif isinstance(self.dataset_path, pd.DataFrame):\n",
        "      self.dataset_name = input(\"Dataset name not found. Please enter dataset name: \")\n",
        "    self.model_savepath = model_savepath\n",
        "    self.nb_name = get('http://172.28.0.2:9000/api/sessions').json()[0]['name'].split(\".\")[0]\n",
        "\n",
        "\n",
        "  def load_split_dataset(self, dataset_path, dropna=False, do_split=True, use_scaler=False):\n",
        "    if isinstance(dataset_path, str): \n",
        "      format = dataset_path.split(\".\")[-1]\n",
        "      valid = {\"csv\", \"xlsx\", \"xls\"}\n",
        "      if format not in valid:\n",
        "        raise ValueError(f\"results: status must be one of {valid}.\")\n",
        "      elif format == \"csv\":\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "      elif (format == \"xlsx\" or format == \"xls\"):\n",
        "        dataset = pd.read_excel(dataset_path)\n",
        "    else:\n",
        "      dataset = dataset_path\n",
        "\n",
        "    print(\"Dataset head\\n\") \n",
        "    print(dataset.head())\n",
        "\n",
        "    X = dataset.drop(self.target_col, axis=1)\n",
        "    y = self.lbl_enc.fit_transform(dataset[self.target_col].values)\n",
        "\n",
        "    #dropping nans\n",
        "    if dropna:\n",
        "      print(\"DROPPING NAN\")\n",
        "      dataset = dataset.dropna(axis=0, how='any')\n",
        "\n",
        "    if use_scaler:\n",
        "      X = self.scaler.fit_transform(X)\n",
        "\n",
        "    \n",
        "    listy = list(self.lbl_enc.inverse_transform(y))\n",
        "    print(\"Dataset class distribution:\")\n",
        "    for i in set(listy):\n",
        "      print(i, listy.count(i))\n",
        "\n",
        "    if do_split:\n",
        "      xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=self.split_size, shuffle=True)\n",
        "      return xtrain, xvalid, ytrain, yvalid\n",
        "    else:\n",
        "      return X, y\n",
        "\n",
        "\n",
        "  def print_cm(self, yvalid, predicted, target_names=[]): \n",
        "    cm = metrics.confusion_matrix(yvalid, predicted)\n",
        "    disp = metrics.ConfusionMatrixDisplay(cm, display_labels=target_names)\n",
        "    disp.plot(xticks_rotation=\"vertical\")\n",
        "\n",
        "  def print_report(self, predicted, yvalid, target_names=None):\n",
        "    report_dict = metrics.classification_report(yvalid, predicted, target_names=[str(x) for x in target_names], output_dict=True)\n",
        "    report_text = metrics.classification_report(yvalid, predicted, target_names=[str(x) for x in target_names])\n",
        "    print(report_text)\n",
        "    self.print_cm(yvalid, predicted, target_names=target_names)\n",
        "    return report_dict\n",
        "\n",
        "  \n",
        "  def load_test_dataset(self, testdataset_path, dropna):\n",
        "    X, y = self.load_split_dataset(testdataset_path, dropna=dropna, do_split=False)\n",
        "    target_names = self.lbl_enc.inverse_transform(list(set(y)))\n",
        "    return X, y, target_names\n",
        "\n",
        "\n",
        "class PublicExpertiment(Experiment):\n",
        "  def __init__(self, dataset_path, split_size, target_col, text_col, model_savepath=MODEL_SAVEPATH, preprocess_dataset=True):\n",
        "    super().__init__(dataset_path, split_size, target_col, model_savepath)\n",
        "    self.text_col = text_col\n",
        "    self.preprocess_dataset = preprocess_dataset\n",
        "\n",
        "\n",
        "class ScikitExperiment(PublicExpertiment):\n",
        "  def __init__(self, dataset_path, split_size, target_col, text_col, algo, model_savepath=MODEL_SAVEPATH, preprocess_dataset=True):\n",
        "    super().__init__(dataset_path=dataset_path, \n",
        "                     split_size=split_size, \n",
        "                     target_col=target_col, \n",
        "                     text_col=text_col, \n",
        "                     model_savepath=model_savepath, \n",
        "                     preprocess_dataset=preprocess_dataset\n",
        "                     )\n",
        "    self.algo = algo\n",
        "\n",
        "\n",
        "  def train(self, dropna=False):\n",
        "    start = time.time()\n",
        "    xtrain, xvalid, ytrain, yvalid = super().load_split_dataset(self.dataset_path, dropna=dropna)\n",
        "    xtrain = xtrain[self.text_col]\n",
        "    xvalid = xvalid[self.text_col]\n",
        "\n",
        "    if self.preprocess_dataset:\n",
        "      xtrain, xvalid = apply_preprocess(xtrain, xvalid)\n",
        "      \n",
        "    clf_pipeline = Pipeline([\n",
        "     ('ctv', CountVectorizer()),\n",
        "     ('tfidf', TfidfTransformer()),\n",
        "     ('clf', self.algo),\n",
        "    ])\n",
        "    print(f\"Fitting pipeline: {clf_pipeline}\")\n",
        "    clf_pipeline.fit(xtrain, ytrain)\n",
        "    end = time.time()\n",
        "    elapsed = round(end - start, 2)\n",
        "    predicted = clf_pipeline.predict(xvalid)\n",
        "    report = super().print_report(predicted, yvalid, target_names=self.lbl_enc.inverse_transform(list(set(yvalid))))\n",
        "\n",
        "    self.save_model(clf_pipeline, self.lbl_enc)\n",
        "    print(\"Time elapsed in seconds: \", round(elapsed, 2))\n",
        "    \n",
        "    log_dict = self.log(self.model_savepath, self.dataset_name, len(xtrain)+len(xvalid), type(self.algo).__name__, elapsed, report)\n",
        "    return log_dict\n",
        "\n",
        "  \n",
        "  def save_model(self, model, lbl_enc):\n",
        "    algo_name = type(self.algo).__name__\n",
        "    experiment_name = self.nb_name+\"__\"+self.dataset_name+\"__\"+algo_name\n",
        "    save_confirmation = input(f\"Save model {experiment_name}? (y/n)\")\n",
        "    if save_confirmation == \"y\":\n",
        "      filepath = f'{self.model_savepath}models/sklearn/{experiment_name}.pkl'\n",
        "      print(f\"Saving model to {filepath}\")\n",
        "      data = {\n",
        "          \"model\": model,\n",
        "          \"lbl_enc\": lbl_enc\n",
        "      }\n",
        "      joblib.dump(data, filepath)\n",
        "    else:\n",
        "      print(\"Model wasn't saved\")\n",
        "\n",
        "\n",
        "  def load_model_and_predict(self, modelpath, X):\n",
        "    data = joblib.load(modelpath, mmap_mode=None)\n",
        "    model = data[\"model\"] #sklearn  \n",
        "    predicted = model.predict(X) \n",
        "    return predicted\n",
        "\n",
        "\n",
        "  def evaluate_on_other_dataset(self, testdataset_path: Union[str, pd.DataFrame], modelpath: str, dropna=False):\n",
        "    start = time.time()\n",
        "    if isinstance(testdataset_path, str):\n",
        "      format = testdataset_path.split(\".\")[-1]\n",
        "      valid = {\"csv\", \"xlsx\", \"xls\"}\n",
        "      if format not in valid:\n",
        "        raise ValueError(f\"results: status must be one of {valid}.\")\n",
        "      elif format == \"csv\":\n",
        "        dataset = pd.read_csv(testdataset_path)\n",
        "      elif (format == \"xlsx\" or format == \"xls\"):\n",
        "        dataset = pd.read_excel(testdataset_path)\n",
        "      X = dataset[self.text_col]\n",
        "      y = dataset[self.target_col]\n",
        "    elif isinstance(testdataset_path, pd.DataFrame):\n",
        "      X = testdataset_path[self.text_col]\n",
        "      y = testdataset_path[self.target_col]\n",
        "    print(X.shape)\n",
        "\n",
        "    if self.preprocess_dataset:\n",
        "      print(\"Preprocessing text...\")\n",
        "      X = X.progress_apply(lambda x: preprocess(x))\n",
        "    target_names = list(set(y))\n",
        "    lbl_enc = joblib.load(modelpath, mmap_mode=None)[\"lbl_enc\"]\n",
        "\n",
        "    predicted = self.load_model_and_predict(modelpath, X)\n",
        "    super().print_report(predicted, lbl_enc.transform(y), target_names)\n",
        "    end = time.time()\n",
        "    elapsed = round(end - start, 2)\n",
        "    print(\"Time elapsed in seconds: \", round(elapsed, 2))\n",
        "\n",
        "\n",
        "  def log(self, savepath, datasetname, dataset_len, algo, elapsed, report):\n",
        "    log_dict = {\n",
        "        \"library_used\": type(self).__name__,\n",
        "        \"dataset_name\": datasetname,\n",
        "        \"dataset_lenght\": dataset_len,\n",
        "        \"algo\": algo,\n",
        "        \"elapsed\": elapsed,\n",
        "        \"metrics_report\": report\n",
        "    }\n",
        "    algo_name = type(self.algo).__name__\n",
        "    experiment_name = self.nb_name+\"__\"+self.dataset_name+\"__\"+algo_name\n",
        "    filepath = f'{savepath}logs/sklearn/{experiment_name}_log.json'\n",
        "    with open(filepath, 'w') as fp:\n",
        "      json.dump(log_dict, fp)\n",
        "      print(\"Log saved to \", filepath)\n",
        "    return log_dict\n",
        "\n",
        "\n",
        "class TFExperiment(PublicExpertiment):\n",
        "  def __init__(self, dataset_path, split_size, text_col, target_col, preprocess_dataset=True, model_savepath=MODEL_SAVEPATH, bert_pretrained_model='bert-large-uncased', bert_encode_maxlen=60):\n",
        "    super().__init__(dataset_path=dataset_path, \n",
        "                     split_size=split_size, \n",
        "                     text_col=text_col, \n",
        "                     target_col=target_col, \n",
        "                     preprocess_dataset=preprocess_dataset, \n",
        "                     model_savepath=model_savepath)\n",
        "    self.bert_pretrained_model = bert_pretrained_model\n",
        "    self.bert_encode_maxlen = bert_encode_maxlen\n",
        "  \n",
        "\n",
        "  def bert_encode(self, data, max_len) :\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "    input_ids = [] \n",
        "    attention_masks = []\n",
        "    \n",
        "    for i in tqdm(range(len(data))):\n",
        "        encoded = bert_tokenizer.encode_plus(data.iloc[i],\n",
        "                                        add_special_tokens=True,\n",
        "                                        max_length=max_len,\n",
        "                                        pad_to_max_length=True,\n",
        "                                        return_attention_mask=True)\n",
        "        \n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "        \n",
        "    return np.array(input_ids),np.array(attention_masks)\n",
        "\n",
        "\n",
        "  def create_model(self, bert_encode_maxlen, bert_pretrained_model, optimizer, loss, metrics):\n",
        "    bert_layers = TFBertModel.from_pretrained(bert_pretrained_model)\n",
        "\n",
        "    input_ids = keras.Input(shape=(bert_encode_maxlen,),dtype='int32',name='input_ids')\n",
        "    attention_masks = keras.Input(shape=(bert_encode_maxlen,),dtype='int32',name='attention_masks')\n",
        "\n",
        "    output = bert_layers([input_ids,attention_masks])\n",
        "    output = output[1]\n",
        "    net = keras.layers.Dense(32,activation='relu')(output)\n",
        "    net = keras.layers.Dropout(0.2)(net)\n",
        "    net = keras.layers.Dense(1,activation='sigmoid')(net)\n",
        "    outputs = net\n",
        "    model = keras.models.Model(inputs = [input_ids,attention_masks],outputs = outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=[metrics])\n",
        "    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(self, bert_encode_maxlen=None, bert_pretrained_model=None, dropna=False, epochs=10, optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics='accuracy', callbacks=[]):\n",
        "    start = time.time()\n",
        "    if bert_encode_maxlen is None:\n",
        "      bert_encode_maxlen = self.bert_encode_maxlen\n",
        "    if bert_pretrained_model is None:\n",
        "      bert_pretrained_model = self.bert_pretrained_model\n",
        "\n",
        "    xtrain, xvalid, ytrain, yvalid = super().load_split_dataset(self.dataset_path, dropna=dropna)\n",
        "    xtrain = xtrain[self.text_col]\n",
        "    xvalid = xvalid[self.text_col]\n",
        "\n",
        "    if self.preprocess_dataset:\n",
        "      xtrain, xvalid = apply_preprocess(xtrain, xvalid)\n",
        "\n",
        "    train_input_ids, train_attention_masks = self.bert_encode(xtrain, bert_encode_maxlen)\n",
        "    val_input_ids, val_attention_masks = self.bert_encode(xvalid, bert_encode_maxlen)\n",
        "\n",
        "    model = self.create_model(bert_encode_maxlen, bert_pretrained_model, optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    history = model.fit(\n",
        "    [train_input_ids, train_attention_masks],\n",
        "    ytrain,\n",
        "    epochs=epochs,\n",
        "    # validation_data=([val_input_ids, val_attention_masks], y_val),\n",
        "    batch_size=32, \n",
        "    # callbacks=callbacks for now no callbacls\n",
        "    )\n",
        "    end = time.time()\n",
        "    elapsed = round(end - start, 2)\n",
        "\n",
        "    predicted = model.predict([val_input_ids, val_attention_masks])\n",
        "    predicted = np.array(list(round(i[0]) for i in predicted))\n",
        "    report = super().print_report(predicted, yvalid, target_names=self.lbl_enc.inverse_transform(list(set(yvalid))))\n",
        "\n",
        "    self.save_model(model, self.lbl_enc)\n",
        "    print(\"Time elapsed in seconds: \", round(elapsed, 2))\n",
        "    \n",
        "    log_dict = self.log(self.model_savepath, self.dataset_name, model, len(xtrain)+len(xvalid), elapsed, bert_encode_maxlen, epochs, bert_pretrained_model, optimizer, report)\n",
        "\n",
        "  \n",
        "  def save_model(self, model, lbl_enc):\n",
        "    algo_name = type(model).__name__\n",
        "    experiment_name = self.nb_name+\"__\"+self.dataset_name+\"__\"+algo_name\n",
        "    save_confirmation = input(f\"Save model {experiment_name}? (y/n)\")\n",
        "    if save_confirmation == \"y\":\n",
        "      filepath = f'{self.model_savepath}models/bert/{experiment_name}.h5'\n",
        "      lbl_enc_path = f'{self.model_savepath}models/bert/{experiment_name}__lbl_enc.pkl'\n",
        "      data = {\n",
        "          \"model\": model,\n",
        "          \"lbl_enc\": lbl_enc\n",
        "      }\n",
        "      print(f\"Saving model to {filepath}\")\n",
        "      # joblib.dump(data, filepath)\n",
        "      joblib.dump(lbl_enc, lbl_enc_path)\n",
        "      model.save(f'{self.model_savepath}models/bert/{experiment_name}.h5')\n",
        "    else:\n",
        "      print(\"Model wasn't saved\")\n",
        "\n",
        "\n",
        "  def load_model_and_predict(self, modelpath, X):\n",
        "    # data = joblib.load(modelpath, mmap_mode=None)\n",
        "    # model = data[\"model\"]\n",
        "    model = load_model(modelpath, custom_objects={'TFBertModel':TFBertModel.from_pretrained(self.bert_pretrained_model)}) \n",
        "    predicted = model.predict(X) \n",
        "    predicted = np.array(list(round(i[0]) for i in predicted))\n",
        "    return predicted\n",
        "\n",
        "\n",
        "  def evaluate_on_other_dataset(self, testdataset_path, modelpath, text_col, fitted_lbl_enc, dropna=False):\n",
        "    start = time.time()\n",
        "    if isinstance(testdataset_path, str):\n",
        "      format = testdataset_path.split(\".\")[-1]\n",
        "      valid = {\"csv\", \"xlsx\", \"xls\"}\n",
        "      if format not in valid:\n",
        "        raise ValueError(f\"results: status must be one of {valid}.\")\n",
        "      elif format == \"csv\":\n",
        "        dataset = pd.read_csv(testdataset_path)\n",
        "      elif (format == \"xlsx\" or format == \"xls\"):\n",
        "        dataset = pd.read_excel(testdataset_path)\n",
        "      X = dataset[self.text_col]\n",
        "      y = dataset[self.target_col]\n",
        "    elif isinstance(testdataset_path, pd.DataFrame):\n",
        "      X = testdataset_path[self.text_col]\n",
        "      y = testdataset_path[self.target_col]\n",
        "      \n",
        "    target_names = list(set(y))\n",
        "    lbl_enc = joblib.load(modelpath, mmap_mode=None)[\"lbl_enc\"]\n",
        "\n",
        "    if self.preprocess_dataset:\n",
        "      print(\"Preprocessing text...\")\n",
        "      X = X.progress_apply(lambda x: preprocess(x))\n",
        "    input_ids, attention_masks = self.bert_encode(X, self.bert_encode_maxlen)\n",
        "\n",
        "    predicted = self.load_model_and_predict(modelpath, [input_ids, attention_masks])\n",
        "    super().print_report(predicted, lbl_enc.transform(y), target_names)\n",
        "    end = time.time()\n",
        "    elapsed = round(end - start, 2)\n",
        "    print(\"Time elapsed in seconds: \", round(elapsed, 2))\n",
        "\n",
        "\n",
        "  def log(self, savepath, datasetname, model, dataset_len, elapsed, bert_encode_maxlen, epochs, bert_pretrained_model, optimizer, report):\n",
        "    log_dict = {\n",
        "        \"library_used\": type(self).__name__,\n",
        "        \"dataset_name\": datasetname,\n",
        "        \"dataset_lenght\": dataset_len,\n",
        "        \"elapsed\": elapsed,\n",
        "        \"bert_encode_maxlen\": bert_encode_maxlen,\n",
        "        \"epochs\": epochs,\n",
        "        \"bert_pretrained_model\": bert_pretrained_model,\n",
        "        \"optimizer\": str(optimizer),\n",
        "        \"metrics_report\": report\n",
        "    }\n",
        "    algo_name = type(model).__name__\n",
        "    experiment_name = self.nb_name+\"__\"+self.dataset_name+\"__\"+algo_name\n",
        "    filepath = f'{savepath}logs/bert/{experiment_name}_log.json'\n",
        "    with open(filepath, 'w') as fp:\n",
        "      json.dump(log_dict, fp)\n",
        "      print(\"Log saved to \", filepath)\n",
        "    return log_dict\n",
        "\n",
        "\n",
        "class StyloExperiment(Experiment):\n",
        "  def __init__(self, dataset_path, split_size, target_col, model_savepath=MODEL_SAVEPATH):\n",
        "    super().__init__(dataset_path=dataset_path, \n",
        "                     split_size=split_size, \n",
        "                     target_col=target_col, \n",
        "                     model_savepath=model_savepath)\n",
        "\n",
        "\n",
        "  def train(self, \n",
        "            epochs=10, \n",
        "            use_scaler=True, \n",
        "            n_layers=1, \n",
        "            n_units_input=51,\n",
        "            n_units_per_layer=None,\n",
        "            dropout_per_layer=None,\n",
        "            activation=\"relu\",\n",
        "            learning_rate=0.0014392587661767942,\n",
        "            optimizer=\"RMSprop\"\n",
        "            ):\n",
        "    start = time.time()\n",
        "    xtrain, xvalid, ytrain, yvalid = super().load_split_dataset(self.dataset_path, use_scaler=use_scaler)\n",
        "\n",
        "    if not n_units_per_layer:\n",
        "      n_units_per_layer = [80]\n",
        "    if not dropout_per_layer:\n",
        "      dropout_per_layer = [0.3203504513234906]\n",
        "\n",
        "    nn_parameters = {\n",
        "      \"n_layers\": n_layers,\n",
        "      \"n_units_input\": n_units_input,\n",
        "      \"activation\": activation,\n",
        "      \"n_units_per_layer\": n_units_per_layer,\n",
        "      \"dropout_per_layer\": dropout_per_layer,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"optimizer\": optimizer\n",
        "    }\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(\n",
        "        Dense(\n",
        "            nn_parameters[\"n_units_input\"], \n",
        "            input_dim=xtrain.shape[1],\n",
        "            activation=nn_parameters[\"activation\"],\n",
        "          )\n",
        "    )\n",
        "    for i in range(nn_parameters[\"n_layers\"]):\n",
        "      model.add(\n",
        "          Dense(\n",
        "            nn_parameters[\"n_units_per_layer\"][i],\n",
        "            activation=nn_parameters[\"activation\"],\n",
        "          )\n",
        "      )\n",
        "      model.add(\n",
        "          Dropout(nn_parameters[\"dropout_per_layer\"][i])\n",
        "      )\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # We compile our model with a sampled learning rate.\n",
        "    learning_rate = nn_parameters[\"learning_rate\"]\n",
        "    optimizer_name = nn_parameters[\"optimizer\"]\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=getattr(keras.optimizers, optimizer_name)(learning_rate=learning_rate),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        xtrain,\n",
        "        ytrain,\n",
        "        batch_size=512, \n",
        "        epochs=epochs,\n",
        "        validation_data=(xvalid, yvalid)\n",
        "    )\n",
        "    end = time.time()\n",
        "    elapsed = round(end - start, 2)\n",
        "\n",
        "    predicted = model.predict(xvalid)\n",
        "    predicted = np.array(list(round(i[0]) for i in predicted))\n",
        "    report = super().print_report(predicted, yvalid, target_names=self.lbl_enc.inverse_transform(list(set(yvalid))))\n",
        "    self.save_model(model, self.scaler, self.lbl_enc)\n",
        "    print(\"Time elapsed in seconds: \", round(elapsed, 2))\n",
        "    \n",
        "    log_dict = self.log(self.model_savepath, self.dataset_name, model, len(xtrain)+len(xvalid), elapsed, epochs, nn_parameters, report)\n",
        "\n",
        "  \n",
        "  def save_model(self, model, scaler, lbl_enc):\n",
        "    algo_name = type(model).__name__\n",
        "    experiment_name = self.nb_name+\"__\"+self.dataset_name+\"__\"+algo_name+\"_stilometria\"\n",
        "    save_confirmation = input(f\"Save model {experiment_name}? (y/n)\")\n",
        "    if save_confirmation == \"y\":\n",
        "      filepath = f'{self.model_savepath}models/stylo/{experiment_name}.pkl'\n",
        "      data = {\n",
        "          \"model\": model,\n",
        "          \"scaler\": scaler,\n",
        "          \"lbl_enc\": lbl_enc\n",
        "      }\n",
        "      print(f\"Saving model to {filepath}\")\n",
        "      joblib.dump(data, filepath)\n",
        "      # model.save(f'{self.model_savepath}{experiment_name}.h5')\n",
        "    else:\n",
        "      print(\"Model wasn't saved\")\n",
        "\n",
        "  def load_model_and_predict(self, modelpath, X):\n",
        "    data = joblib.load(modelpath, mmap_mode=None)\n",
        "    model = data[\"model\"]\n",
        "    predicted = model.predict(X) \n",
        "    predicted = np.array(list(round(i[0]) for i in predicted))\n",
        "    return predicted\n",
        "\n",
        "\n",
        "  def evaluate_on_other_dataset(self, testdataset_path, modelpath, dropna=False, use_scaler=True):\n",
        "    start = time.time()\n",
        "    if isinstance(testdataset_path, str):\n",
        "      format = testdataset_path.split(\".\")[-1]\n",
        "      valid = {\"csv\", \"xlsx\", \"xls\"}\n",
        "      if format not in valid:\n",
        "        raise ValueError(f\"results: status must be one of {valid}.\")\n",
        "      elif format == \"csv\":\n",
        "        testdataset = pd.read_csv(testdataset_path)\n",
        "      elif (format == \"xlsx\" or format == \"xls\"):\n",
        "        testdataset = pd.read_excel(testdataset_path)\n",
        "      X = testdataset.drop(self.target_col, axis=1)\n",
        "      y = testdataset[self.target_col]\n",
        "    elif isinstance(testdataset_path, pd.DataFrame):\n",
        "      X = testdataset_path.drop(self.target_col, axis=1)\n",
        "      y = testdataset_path[self.target_col]\n",
        "\n",
        "    target_names = list(set(y))\n",
        "    lbl_enc = joblib.load(modelpath, mmap_mode=None)[\"lbl_enc\"]\n",
        "    scaler = joblib.load(modelpath, mmap_mode=None)[\"scaler\"]\n",
        "\n",
        "    if use_scaler:\n",
        "      X = scaler.transform(X)\n",
        "\n",
        "    predicted = self.load_model_and_predict(modelpath, X)\n",
        "    super().print_report(predicted, lbl_enc.transform(y), target_names)\n",
        "    end = time.time()\n",
        "    elapsed = round(end - start, 2)\n",
        "    print(\"Time elapsed in seconds: \", round(elapsed, 2))\n",
        "\n",
        "\n",
        "  def log(self, savepath, datasetname, model, dataset_len, elapsed, epochs, nn_parameters, report):\n",
        "    log_dict = {\n",
        "        \"library_used\": type(self).__name__,\n",
        "        \"dataset_name\": datasetname,\n",
        "        \"dataset_lenght\": dataset_len,\n",
        "        \"elapsed\": elapsed,\n",
        "        \"epochs\": epochs,\n",
        "        \"nueral_net_parameters\": nn_parameters,\n",
        "        \"metrics_report\": report\n",
        "    }\n",
        "    algo_name = type(model).__name__\n",
        "    experiment_name = self.nb_name+\"__\"+self.dataset_name+\"__\"+algo_name+\"_stilometria\"\n",
        "    filepath = f'{savepath}logs/stylo/{experiment_name}_log.json'\n",
        "    with open(filepath, 'w') as fp:\n",
        "      json.dump(log_dict, fp)\n",
        "      print(\"Log saved to \", filepath)\n",
        "    return log_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp = StyloExperiment()"
      ],
      "metadata": {
        "id": "KtamVrBWTx_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}